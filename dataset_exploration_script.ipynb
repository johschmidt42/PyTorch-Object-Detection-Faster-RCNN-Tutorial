{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9df77e-b310-4b7c-9d9c-1ea9268d2758",
   "metadata": {},
   "source": [
    "## Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8d6d30-0de4-4e75-a628-219a487b2301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from napari.layers import Shapes\n",
    "from overrides import overrides\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from pytorch_faster_rcnn_tutorial.anchor_generator import get_anchor_boxes\n",
    "from pytorch_faster_rcnn_tutorial.transformations import re_normalize\n",
    "from pytorch_faster_rcnn_tutorial.utils import stats_dataset\n",
    "from pytorch_faster_rcnn_tutorial.viewers.dataset_viewer import (\n",
    "    DatasetViewer,\n",
    "    make_bbox_napari,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe99af5-2776-4ad3-a983-cd97eaa374d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d:%(funcName)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb489c9-60fa-4678-b863-36c20867d6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root directory\n",
    "data_path = pathlib.Path(\"src/pytorch_faster_rcnn_tutorial/data/heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a669bbb-d077-4b95-9bb3-e1c726615091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-08 16:24:13 - INFO - utils.py:27:get_filenames_of_path: Found 20 files in src/pytorch_faster_rcnn_tutorial/data/heads/input\n",
      "2023-06-08 16:24:13 - INFO - utils.py:27:get_filenames_of_path: Found 20 files in src/pytorch_faster_rcnn_tutorial/data/heads/target\n"
     ]
    }
   ],
   "source": [
    "# input and target files\n",
    "inputs: List[pathlib.Path] = get_filenames_of_path(data_path / \"input\")\n",
    "targets: List[pathlib.Path] = get_filenames_of_path(data_path / \"target\")\n",
    "\n",
    "inputs.sort()\n",
    "targets.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd324587-2d3a-40b6-8517-caf558e6be05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mapping\n",
    "mapping = {\n",
    "    \"head\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f100e8b9-e650-4adb-9ecc-aa1255ce607a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforms\n",
    "transforms: ComposeDouble = ComposeDouble(\n",
    "    [\n",
    "        Clip(),\n",
    "        # AlbumentationWrapper(albumentation=A.HorizontalFlip(p=0.5)),\n",
    "        # AlbumentationWrapper(albumentation=A.RandomScale(p=0.5, scale_limit=0.5)),\n",
    "        # AlbumentationWrapper(albumentation=A.VerticalFlip(p=0.5)),\n",
    "        FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2fb91b8-c3c1-42bb-8df6-67886f32e02a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset building\n",
    "dataset: ObjectDetectionDataSet = ObjectDetectionDataSet(\n",
    "    inputs=inputs,\n",
    "    targets=targets,\n",
    "    transform=transforms,\n",
    "    use_cache=False,\n",
    "    convert_to_format=None,\n",
    "    mapping=mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fce20d-f33b-4916-a5be-5ceead59060e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-08 16:24:19 - INFO - object_detection_viewer.py:43:get_data: Input sample: 001.jpg\n",
      "Shape: torch.Size([3, 710, 1024])\n",
      "2023-06-08 16:24:19 - INFO - object_detection_viewer.py:50:get_data: Transformed input sample: 001.jpg\n",
      "Shape: torch.Size([3, 736, 1024])\n",
      "2023-06-08 16:24:19 - INFO - object_detection_viewer.py:65:get_target: Target sample: 001.json\n",
      "{'boxes': tensor([[ 14, 217, 277, 531],\n",
      "        [199,  81, 396, 287],\n",
      "        [386,   2, 588, 247],\n",
      "        [306, 251, 510, 521],\n",
      "        [525, 266, 741, 554],\n",
      "        [723, 116, 925, 432]]), 'labels': tensor([1, 1, 1, 1, 1, 1])}\n",
      "2023-06-08 16:24:19 - INFO - object_detection_viewer.py:72:get_target: Transformed target sample: 001.json\n",
      "{'boxes': tensor([[ 14., 217., 277., 531.],\n",
      "        [199.,  81., 396., 287.],\n",
      "        [386.,   2., 588., 247.],\n",
      "        [306., 251., 510., 521.],\n",
      "        [525., 266., 741., 554.],\n",
      "        [723., 116., 925., 432.]]), 'labels': tensor([1, 1, 1, 1, 1, 1])}\n",
      "2023-06-08 16:24:20 - INFO - object_detection_viewer.py:43:get_data: Input sample: 002.jpg\n",
      "Shape: torch.Size([3, 440, 660])\n",
      "2023-06-08 16:24:20 - INFO - object_detection_viewer.py:50:get_data: Transformed input sample: 002.jpg\n",
      "Shape: torch.Size([3, 704, 1024])\n",
      "2023-06-08 16:24:20 - INFO - object_detection_viewer.py:65:get_target: Target sample: 002.json\n",
      "{'boxes': tensor([[163,  63, 225, 138],\n",
      "        [131, 133, 200, 224],\n",
      "        [245,  39, 302, 107],\n",
      "        [305,  23, 360,  94],\n",
      "        [378,   3, 433,  64],\n",
      "        [398,  45, 455, 113],\n",
      "        [228, 142, 295, 243],\n",
      "        [174, 272, 261, 365],\n",
      "        [266, 283, 334, 366],\n",
      "        [317, 280, 403, 374],\n",
      "        [291, 147, 349, 239],\n",
      "        [341, 138, 416, 235],\n",
      "        [398, 137, 442, 204],\n",
      "        [430, 130, 499, 191]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "2023-06-08 16:24:20 - INFO - object_detection_viewer.py:72:get_target: Transformed target sample: 002.json\n",
      "{'boxes': tensor([[252.6500,  97.6500, 348.7500, 213.9000],\n",
      "        [203.0500, 206.1500, 310.0000, 347.2000],\n",
      "        [379.7500,  60.4500, 468.1000, 165.8500],\n",
      "        [472.7500,  35.6500, 558.0000, 145.7000],\n",
      "        [585.9000,   4.6500, 671.1500,  99.2000],\n",
      "        [616.9000,  69.7500, 705.2500, 175.1500],\n",
      "        [353.4000, 220.1000, 457.2500, 376.6500],\n",
      "        [269.7000, 421.6000, 404.5500, 565.7500],\n",
      "        [412.3000, 438.6500, 517.7000, 567.3000],\n",
      "        [491.3500, 434.0000, 624.6500, 579.7000],\n",
      "        [451.0500, 227.8500, 540.9500, 370.4500],\n",
      "        [528.5500, 213.9000, 644.8000, 364.2500],\n",
      "        [616.9000, 212.3500, 685.1000, 316.2000],\n",
      "        [666.5000, 201.5000, 773.4500, 296.0500]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "2023-06-08 16:24:22 - INFO - object_detection_viewer.py:43:get_data: Input sample: 003.png\n",
      "Shape: torch.Size([3, 565, 581])\n",
      "2023-06-08 16:24:22 - INFO - object_detection_viewer.py:50:get_data: Transformed input sample: 003.png\n",
      "Shape: torch.Size([3, 1024, 1024])\n",
      "2023-06-08 16:24:22 - INFO - object_detection_viewer.py:65:get_target: Target sample: 003.json\n",
      "{'boxes': tensor([[ 88,  93, 171, 204],\n",
      "        [ 17, 214, 190, 367],\n",
      "        [205, 106, 353, 290],\n",
      "        [301,  13, 379,  98],\n",
      "        [437,  86, 580, 428]]), 'labels': tensor([1, 1, 1, 1, 1])}\n",
      "2023-06-08 16:24:22 - INFO - object_detection_viewer.py:72:get_target: Transformed target sample: 003.json\n",
      "{'boxes': tensor([[ 154.9466,  163.7788,  301.0895,  359.2566],\n",
      "        [  29.9329,  376.8672,  334.5439,  646.3097],\n",
      "        [ 360.9553,  186.6726,  621.5474,  510.7079],\n",
      "        [ 529.9880,   22.8938,  667.3270,  172.5841],\n",
      "        [ 769.4509,  151.4513, 1021.2393,  753.7345]]), 'labels': tensor([1, 1, 1, 1, 1])}\n",
      "2023-06-08 16:24:24 - INFO - object_detection_viewer.py:43:get_data: Input sample: 004.jpg\n",
      "Shape: torch.Size([3, 570, 800])\n",
      "2023-06-08 16:24:24 - INFO - object_detection_viewer.py:50:get_data: Transformed input sample: 004.jpg\n",
      "Shape: torch.Size([3, 736, 1024])\n",
      "2023-06-08 16:24:24 - INFO - object_detection_viewer.py:65:get_target: Target sample: 004.json\n",
      "{'boxes': tensor([[289,  99, 419, 271],\n",
      "        [387, 163, 485, 302],\n",
      "        [480, 245, 576, 366],\n",
      "        [526, 185, 657, 304],\n",
      "        [588,  51, 781, 236]]), 'labels': tensor([1, 1, 1, 1, 1])}\n",
      "2023-06-08 16:24:24 - INFO - object_detection_viewer.py:72:get_target: Transformed target sample: 004.json\n",
      "{'boxes': tensor([[369.5587, 126.6158, 535.7962, 346.5947],\n",
      "        [494.8762, 208.4684, 620.1937, 386.2421],\n",
      "        [613.8000, 313.3421, 736.5599, 468.0947],\n",
      "        [672.6225, 236.6053, 840.1387, 388.8000],\n",
      "        [751.9050,  65.2263, 998.7037, 301.8316]]), 'labels': tensor([1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# visualize dataset\n",
    "color_mapping: Dict[int, str] = {\n",
    "    1: \"red\",\n",
    "}\n",
    "\n",
    "transform: GeneralizedRCNNTransform = GeneralizedRCNNTransform(\n",
    "    min_size=1024,\n",
    "    max_size=1024,\n",
    "    image_mean=[0.485, 0.456, 0.406],\n",
    "    image_std=[0.229, 0.224, 0.225],\n",
    ")\n",
    "\n",
    "object_detection_viewer_rcnn: ObjectDetectionViewer = ObjectDetectionViewer(\n",
    "    dataset=dataset, color_mapping=color_mapping, rcnn_transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6f8b5-9797-49e6-b305-6a72fc964ba1",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f086e511-2869-49a2-88e1-3cd45d27e37b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_height': tensor([1000.,  710.,  440.,  565.,  570.,  417.,  412.,  333.,  450.,  533.,\n",
       "          667.,  408.,  408.,  408.,  394., 1200.,  700.,  422.,  405.,  416.]),\n",
       " 'image_width': tensor([1000., 1024.,  660.,  581.,  800.,  625.,  550.,  500.,  338.,  800.,\n",
       "         1000.,  612.,  612.,  612.,  526., 1200., 1100.,  759.,  720.,  626.]),\n",
       " 'image_mean': tensor([0.6344, 0.4143, 0.6635, 0.4409, 0.7367, 0.4340, 0.5066, 0.4894, 0.4830,\n",
       "         0.5201, 0.5292, 0.4543, 0.4776, 0.5529, 0.4662, 0.4136, 0.4097, 0.5390,\n",
       "         0.3745, 0.5882]),\n",
       " 'image_std': tensor([0.2600, 0.2683, 0.3109, 0.2454, 0.2810, 0.3352, 0.2604, 0.3031, 0.3133,\n",
       "         0.2383, 0.2960, 0.3023, 0.2607, 0.2253, 0.3302, 0.2489, 0.3026, 0.3484,\n",
       "         0.2674, 0.2747]),\n",
       " 'boxes_height': tensor([110, 119, 114, 113, 124, 120, 109, 118, 119, 115, 114, 117, 119, 113,\n",
       "         138, 120, 115, 106, 103, 121, 115, 115, 124, 114, 144, 263, 197, 202,\n",
       "         204, 216, 202,  62,  69,  57,  55,  55,  57,  67,  87,  68,  86,  58,\n",
       "          75,  44,  69,  83, 173, 148,  78, 143, 130,  98,  96, 131, 193,  99,\n",
       "          83,  78, 103,  81,  96, 107,  87,  77,  77,  63, 159, 141, 152,  54,\n",
       "          64,  82,  63,  73,  90,  71, 115, 111,  99,  71, 102, 134,  66, 102,\n",
       "         223, 103, 120,  93, 130, 112, 119, 102, 100,  81,  65,  92, 101,  73,\n",
       "         103,  77,  80,  73, 107,  79,  59,  59,  64,  70,  59,  75, 118, 109,\n",
       "          92,  81,  86, 127, 118,  67,  55,  56,  56,  74,  91,  72, 137, 113,\n",
       "         111,  81, 105, 147, 167, 156, 138,  80, 187, 100, 126, 247, 231,  94,\n",
       "         133, 166, 265, 162, 148, 135, 102, 120, 116, 110, 117, 154, 128, 150,\n",
       "         198, 119,  89, 107,  88, 112, 185,  55, 118,  80,  93,  95, 120, 165,\n",
       "         109,  88,  68,  99,  77, 109,  79,  79,  82]),\n",
       " 'boxes_width': tensor([140, 130, 135, 147, 147, 130, 141, 135, 160, 138, 135, 138, 139, 143,\n",
       "         144, 134, 139, 134, 137, 156, 128, 148, 136, 138, 155, 314, 206, 245,\n",
       "         270, 288, 316,  75,  91,  68,  71,  61,  68, 101,  93,  83,  94,  92,\n",
       "          97,  67,  61, 111, 153, 184,  85, 342, 172, 139, 121, 119, 185, 141,\n",
       "         107,  59,  81, 108, 111, 113, 116, 105,  96,  42, 204, 170, 216, 103,\n",
       "          78, 101,  78, 117, 145, 101, 176, 161, 139, 108,  97, 176,  81, 132,\n",
       "         343, 128, 177, 121, 170, 157, 129, 106, 133, 110,  97, 116,  99,  90,\n",
       "         106,  94, 104,  96, 114, 101,  80,  71,  75,  96,  89,  95, 130, 174,\n",
       "         124, 105, 116, 155, 156,  92,  69,  73,  70,  92, 131, 101, 163, 142,\n",
       "          94,  98, 110, 118, 196, 187, 176, 137, 210, 194, 161, 366, 326, 210,\n",
       "         135, 226, 407, 254, 196, 158, 114, 162, 156, 130, 148, 221, 170, 178,\n",
       "         271, 127,  99, 113,  96, 138, 226, 144, 151, 106, 109, 136, 168, 209,\n",
       "         154,  89,  94, 100,  88, 128, 115, 116, 124]),\n",
       " 'boxes_num': tensor([25.,  6., 14.,  5.,  5., 11.,  3., 10.,  5.,  6., 13.,  8.,  6.,  7.,\n",
       "         10.,  5., 16.,  6., 12.,  4.]),\n",
       " 'boxes_area': tensor([ 15400,  15470,  15390,  16611,  18228,  15600,  15369,  15930,  19040,\n",
       "          15870,  15390,  16146,  16541,  16159,  19872,  16080,  15985,  14204,\n",
       "          14111,  18876,  14720,  17020,  16864,  15732,  22320,  82582,  40582,\n",
       "          49490,  55080,  62208,  63832,   4650,   6279,   3876,   3905,   3355,\n",
       "           3876,   6767,   8091,   5644,   8084,   5336,   7275,   2948,   4209,\n",
       "           9213,  26469,  27232,   6630,  48906,  22360,  13622,  11616,  15589,\n",
       "          35705,  13959,   8881,   4602,   8343,   8748,  10656,  12091,  10092,\n",
       "           8085,   7392,   2646,  32436,  23970,  32832,   5562,   4992,   8282,\n",
       "           4914,   8541,  13050,   7171,  20240,  17871,  13761,   7668,   9894,\n",
       "          23584,   5346,  13464,  76489,  13184,  21240,  11253,  22100,  17584,\n",
       "          15351,  10812,  13300,   8910,   6305,  10672,   9999,   6570,  10918,\n",
       "           7238,   8320,   7008,  12198,   7979,   4720,   4189,   4800,   6720,\n",
       "           5251,   7125,  15340,  18966,  11408,   8505,   9976,  19685,  18408,\n",
       "           6164,   3795,   4088,   3920,   6808,  11921,   7272,  22331,  16046,\n",
       "          10434,   7938,  11550,  17346,  32732,  29172,  24288,  10960,  39270,\n",
       "          19400,  20286,  90402,  75306,  19740,  17955,  37516, 107855,  41148,\n",
       "          29008,  21330,  11628,  19440,  18096,  14300,  17316,  34034,  21760,\n",
       "          26700,  53658,  15113,   8811,  12091,   8448,  15456,  41810,   7920,\n",
       "          17818,   8480,  10137,  12920,  20160,  34485,  16786,   7832,   6392,\n",
       "           9900,   6776,  13952,   9085,   9164,  10168])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = stats_dataset(dataset)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4415bc8-eecf-4dff-b404-71febd6d7b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_height': tensor([1024.,  736.,  704., 1024.,  736.,  704.,  768.,  704., 1024.,  704.,\n",
       "          704.,  704.,  704.,  704.,  768., 1024.,  672.,  576.,  576.,  704.]),\n",
       " 'image_width': tensor([1024., 1024., 1024., 1024., 1024., 1024., 1024., 1024.,  800., 1024.,\n",
       "         1024., 1024., 1024., 1024., 1024., 1024., 1024., 1024., 1024., 1024.]),\n",
       " 'image_mean': tensor([ 0.8202, -0.1485,  0.9195, -0.0364,  1.2597, -0.0650,  0.2541,  0.1718,\n",
       "          0.1446,  0.3035,  0.3443,  0.0213,  0.1232,  0.4450,  0.0727, -0.1546,\n",
       "         -0.1703,  0.3933, -0.3299,  0.5940]),\n",
       " 'image_std': tensor([1.1306, 1.1575, 1.3574, 1.0315, 1.2335, 1.4332, 1.1415, 1.2923, 1.3503,\n",
       "         0.9977, 1.2812, 1.2765, 1.1355, 0.9675, 1.3983, 1.1207, 1.2861, 1.5048,\n",
       "         1.1773, 1.1885]),\n",
       " 'boxes_height': tensor([112.6400, 121.8560, 116.7360, 115.7120, 126.9760, 122.8800, 111.6160,\n",
       "         120.8320, 121.8560, 117.7600, 116.7360, 119.8080, 121.8560, 115.7120,\n",
       "         141.3120, 122.8800, 117.7600, 108.5440, 105.4720, 123.9040, 117.7600,\n",
       "         117.7600, 126.9760, 116.7360, 147.4560, 263.0000, 197.0000, 202.0000,\n",
       "         204.0000, 216.0000, 202.0000,  96.1000, 106.9500,  88.3500,  85.2500,\n",
       "          85.2500,  88.3500, 103.8500, 134.8500, 105.4000, 133.3000,  89.9000,\n",
       "         116.2500,  68.2000, 106.9500, 146.1429, 304.6110, 260.5921, 137.3391,\n",
       "         251.7883, 166.2375, 125.3175, 122.7599, 167.5162, 246.7988, 162.0432,\n",
       "         135.8544, 127.6704, 168.5904, 132.5808, 157.1328, 175.1376, 142.4016,\n",
       "         126.0336, 126.0336, 103.1184, 296.0291, 262.5164, 282.9964, 110.5920,\n",
       "         131.0720, 167.9360, 129.0240, 149.5040, 184.3200, 145.4080, 235.5200,\n",
       "         227.3280, 202.7520, 161.5355, 232.0651, 304.8698, 150.1598, 232.0651,\n",
       "         285.1612, 131.7112, 153.4500, 118.9237, 166.2375, 143.2200, 121.8560,\n",
       "         104.4480, 102.4000,  82.9440,  66.5600,  94.2080, 103.4240,  74.7520,\n",
       "         105.4720,  78.8480,  81.9200,  74.7520, 109.5680, 132.1830,  98.7190,\n",
       "          98.7190, 107.0850, 117.1242,  98.7189, 125.4902, 197.4379, 182.3791,\n",
       "         153.9346, 135.5294, 143.8954, 212.4967, 197.4379, 112.1046,  92.0261,\n",
       "          93.6993,  93.6993, 123.8170, 152.2614, 120.4706, 266.4468, 219.7700,\n",
       "         215.8802, 157.5342, 204.2111, 285.8954, 324.7928, 303.3992, 268.3917,\n",
       "         155.5894, 159.5733,  85.3333, 107.5200, 210.7733, 197.1200,  87.5055,\n",
       "         123.8109, 154.5309, 246.6909, 150.8073, 137.7745, 125.6727,  94.9528,\n",
       "         111.7090, 107.9855, 102.4000, 108.9164, 143.3600, 119.1564, 139.6364,\n",
       "         184.3200, 160.5481, 120.0738, 144.3584, 118.7246, 151.1041, 249.5916,\n",
       "          78.2222, 167.8222, 113.7778, 132.2667, 135.1111, 170.6667, 234.6667,\n",
       "         155.0222, 125.1556,  96.7111, 140.8000, 109.5111, 178.1262, 129.1006,\n",
       "         129.1006, 134.0032]),\n",
       " 'boxes_width': tensor([143.3600, 133.1200, 138.2400, 150.5280, 150.5280, 133.1200, 144.3840,\n",
       "         138.2400, 163.8400, 141.3120, 138.2401, 141.3120, 142.3360, 146.4320,\n",
       "         147.4560, 137.2160, 142.3360, 137.2160, 140.2880, 159.7440, 131.0720,\n",
       "         151.5520, 139.2640, 141.3120, 158.7200, 314.0000, 206.0000, 245.0000,\n",
       "         270.0000, 288.0000, 316.0000, 116.2500, 141.0500, 105.4000, 110.0500,\n",
       "          94.5500, 105.4000, 156.5500, 144.1500, 128.6500, 145.7000, 142.6000,\n",
       "         150.3500, 103.8500,  94.5500, 195.4779, 269.4424, 324.0354, 149.6903,\n",
       "         602.2832, 219.9789, 177.7737, 154.7526, 152.1947, 236.6053, 230.9425,\n",
       "         175.2542,  96.6355, 132.6691, 176.8921, 181.8058, 185.0815, 189.9952,\n",
       "         171.9784, 157.2374,  68.7914, 379.7767, 316.4806, 402.1165, 210.6396,\n",
       "         159.5135, 206.5496, 159.5135, 239.2703, 296.5316, 206.5496, 359.9279,\n",
       "         329.2523, 284.2613, 245.7600, 220.7289, 400.4978, 184.3200, 300.3734,\n",
       "         438.8856, 163.7824, 226.4803, 154.8255, 217.5235, 200.8893, 132.0945,\n",
       "         108.5427, 136.1904, 112.6387,  99.3268, 118.7826, 101.3748,  92.1589,\n",
       "         108.5427,  96.2549, 106.4948,  98.3029, 116.7346, 168.8284, 133.7255,\n",
       "         118.6814, 125.3677, 160.4706, 148.7696, 158.7990, 217.3039, 290.8530,\n",
       "         207.2745, 175.5147, 193.9020, 259.0931, 260.7647, 153.7843, 115.3382,\n",
       "         122.0245, 117.0098, 153.7843, 218.9755, 168.8284, 317.3122, 276.4315,\n",
       "         182.9898, 190.7766, 214.1371, 229.7107, 381.5533, 364.0330, 342.6193,\n",
       "         266.6980, 179.2000, 165.5467, 137.3867, 312.3200, 278.1867, 195.3000,\n",
       "         125.5500, 210.1800, 378.5100, 236.2200, 182.2800, 146.9400, 106.0200,\n",
       "         150.6600, 145.0800, 120.9000, 137.6400, 205.5300, 158.1000, 165.5400,\n",
       "         252.0300, 171.2393, 133.4858, 152.3626, 129.4408, 186.0711, 304.7251,\n",
       "         204.8000, 214.7556, 150.7556, 155.0222, 193.4222, 238.9333, 297.2444,\n",
       "         219.0222, 126.5778, 133.6889, 142.2222, 125.1556, 209.2308, 187.9808,\n",
       "         189.6154, 202.6923]),\n",
       " 'boxes_num': tensor([25.,  6., 14.,  5.,  5., 11.,  3., 10.,  5.,  6., 13.,  8.,  6.,  7.,\n",
       "         10.,  5., 16.,  6., 12.,  4.]),\n",
       " 'boxes_area': tensor([ 16148.0703,  16221.4727,  16137.5898,  17417.8945,  19113.4453,\n",
       "          16357.7871,  16115.5625,  16703.8223,  19964.8887,  16640.9043,\n",
       "          16137.5918,  16930.3105,  17344.4941,  16943.9453,  20837.2988,\n",
       "          16861.1035,  16761.4883,  14893.9746,  14796.4609,  19792.9258,\n",
       "          15435.0381,  17846.7656,  17683.1855,  16496.2031,  23404.2109,\n",
       "          82582.0000,  40582.0000,  49490.0000,  55080.0000,  62208.0000,\n",
       "          63832.0000,  11171.6260,  15085.2979,   9312.0869,   9381.7627,\n",
       "           8060.3872,   9312.0938,  16257.7188,  19438.6309,  13559.7129,\n",
       "          19421.8105,  12819.7422,  17478.1875,   7082.5708,  10112.1162,\n",
       "          28567.6953,  82075.1406,  84441.0625,  20558.3184, 151647.8750,\n",
       "          36568.7383,  22278.1543,  18997.4238,  25495.0879,  58393.8867,\n",
       "          37422.6562,  23809.0508,  12337.4941,  22366.7305,  23452.4961,\n",
       "          28567.6543,  32414.7441,  27055.6230,  21675.0645,  19817.1914,\n",
       "           7093.6562, 112424.9531,  83081.3438, 113797.5156,  23295.0605,\n",
       "          20907.7578,  34687.1094,  20581.0703,  35771.8672,  54656.6992,\n",
       "          30033.9648,  84770.2188,  74848.2578,  57634.5430,  39698.9648,\n",
       "          51223.4688, 122099.6875,  27677.4473,  69706.1797, 125153.1406,\n",
       "          21571.9785,  34753.4062,  18412.4238,  36160.5547,  28771.3730,\n",
       "          16096.5020,  11337.0723,  13945.9014,   9342.7031,   6611.1934,\n",
       "          11190.2715,  10484.5869,   6889.0649,  11448.2168,   7589.5049,\n",
       "           8724.0479,   7348.3364,  12790.3779,  22316.2500,  13201.2422,\n",
       "          11716.1025,  13424.9951,  18794.9922,  14686.3770,  19927.7168,\n",
       "          42904.0352,  53045.5000,  31906.7285,  23787.4062,  27901.6094,\n",
       "          55056.4375,  51484.8438,  17239.9258,  10614.1309,  11433.6162,\n",
       "          10963.7412,  19041.1035,  33341.5195,  20338.8691,  84546.8047,\n",
       "          60751.3359,  39503.8906,  30053.8418,  43729.1562,  65673.2344,\n",
       "         123925.7500, 110447.3281,  91956.1641,  41495.3633,  28595.5410,\n",
       "          14126.6504,  14771.8164,  65828.7344,  54836.1641,  17089.8164,\n",
       "          15544.4609,  32479.3086,  93374.9688,  35623.6875,  25113.5430,\n",
       "          18466.3516,  10066.8916,  16830.0859,  15666.5322,  12380.1553,\n",
       "          14991.2490,  29464.7754,  18838.6211,  23115.3984,  46454.1719,\n",
       "          27492.1484,  16028.1416,  21994.8105,  15367.8086,  28116.1094,\n",
       "          76056.8359,  16019.9111,  36040.7578,  17152.6328,  20504.2715,\n",
       "          26133.4922,  40777.9570,  69753.3594,  33953.3086,  15841.9141,\n",
       "          12929.2021,  20024.8887,  13705.9238,  37269.4844,  24268.4395,\n",
       "          24479.4707,  27161.4141])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = GeneralizedRCNNTransform(\n",
    "    min_size=1024,\n",
    "    max_size=1024,\n",
    "    image_mean=[0.485, 0.456, 0.406],\n",
    "    image_std=[0.229, 0.224, 0.225],\n",
    ")\n",
    "\n",
    "stats_transform = stats_dataset(dataset, transform)\n",
    "stats_transform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
